{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Notes Index \u00b6 Data Management \u00b6 Snowflake-Vs.-Databricks A tabular comparison between snowflake and databricks. DevOps \u00b6 About Docker A POC to develop and deploy docker containers locally and then leverage CI tool to build the project to create docker image that is pushed into remote image registry.","title":"Notes Index"},{"location":"#notes-index","text":"","title":"Notes Index"},{"location":"#data-management","text":"Snowflake-Vs.-Databricks A tabular comparison between snowflake and databricks.","title":"Data Management"},{"location":"#devops","text":"About Docker A POC to develop and deploy docker containers locally and then leverage CI tool to build the project to create docker image that is pushed into remote image registry.","title":"DevOps"},{"location":"About%20Me/Abhishek%27s%20Profile/","text":"","title":"Abhishek's Profile"},{"location":"Container%20and%20Orchestration/About%20Docker/","text":"Docker Notes \u00b6 These notes are based on the LinkedIn course - Docker for developers by Emmanuel Henri - Docker for developers (linkedin.com) Need for Docker \u00b6 Docker is needed in the scenarios where you need to setup new projects with the same settings and configuration across different servers, desktops. Docker provides container where all the project specific settings, its inversion of control and the resources required for the code to run are packaged within the container. A Container is the location where all the resources required by the application exists. The container itself is immutable and is static therefore it requires volume to store the data. It is external disk to which the data is stored. This is required in case of running database as containers that can then store the data on external volumes. In addition, container by itself has no idea about the external environment and the networking. therefore for the container to talk to external environment you will need to configure the networking to enable it to talk to one-another. Core Components of Container based Development \u00b6 Tools Utilized in Docker based Development \u00b6 flowchart LR id1[Visual Studio Code] id2[Docker Desktop] id3[Kubernetes/Kubectl] id4[Minikube] id5[Docker Compose] id6[Docker Swarm] id7[Github] id8[Travis CI] id9[Docker hub] id10[GCP or AWS Kubernetes Cluster] Developer -- Coding --> id1 id1 -- Create Image / build --> id2 id1 -- Code commit --> id7 id7 -- CI pipeline --> id8 id8 -- push image --> id9 id3 --> id4 id9 -- pull image from repo -->id4 id5 -- stack deployment --> id6 id1 --> id5 id9 -- production deployment --> id10 Step 1: Docker Desktop Installation on Windows \u00b6 when installing docker desktop, download the docker for windows. This might require installation of an older version of WSL to run it on the desktop. This is done from Microsoft website. WSL stands for windows subsystem for Linux. Using this you are able to run a Linux environment in windows. Manual installation steps for older versions of WSL | Microsoft Docs The commands for docker are available at Docker-Commands Once docker desktop is installed a simple docker based POC conducted can be performed following steps mentioned in Simple Docker Project These notes are based on my understanding of the topic. Some of the content and images refers to the online articles, learning courses I attended. I encourage you to lookup the links provided to develop your own understanding of it at deeper level. The intention of these notes is to serve as reference marker to other sites, online courses, books that I found useful while learning about it. If you would like to share any thoughts or give comments please feel free to reach me @ abhishekgupta86@gmail.com or @ www.linkedin.com/in/abhishekgupta86","title":"Docker Notes"},{"location":"Container%20and%20Orchestration/About%20Docker/#docker-notes","text":"These notes are based on the LinkedIn course - Docker for developers by Emmanuel Henri - Docker for developers (linkedin.com)","title":"Docker Notes"},{"location":"Container%20and%20Orchestration/About%20Docker/#need-for-docker","text":"Docker is needed in the scenarios where you need to setup new projects with the same settings and configuration across different servers, desktops. Docker provides container where all the project specific settings, its inversion of control and the resources required for the code to run are packaged within the container. A Container is the location where all the resources required by the application exists. The container itself is immutable and is static therefore it requires volume to store the data. It is external disk to which the data is stored. This is required in case of running database as containers that can then store the data on external volumes. In addition, container by itself has no idea about the external environment and the networking. therefore for the container to talk to external environment you will need to configure the networking to enable it to talk to one-another.","title":"Need for Docker"},{"location":"Container%20and%20Orchestration/About%20Docker/#core-components-of-container-based-development","text":"","title":"Core Components of Container based Development"},{"location":"Container%20and%20Orchestration/About%20Docker/#tools-utilized-in-docker-based-development","text":"flowchart LR id1[Visual Studio Code] id2[Docker Desktop] id3[Kubernetes/Kubectl] id4[Minikube] id5[Docker Compose] id6[Docker Swarm] id7[Github] id8[Travis CI] id9[Docker hub] id10[GCP or AWS Kubernetes Cluster] Developer -- Coding --> id1 id1 -- Create Image / build --> id2 id1 -- Code commit --> id7 id7 -- CI pipeline --> id8 id8 -- push image --> id9 id3 --> id4 id9 -- pull image from repo -->id4 id5 -- stack deployment --> id6 id1 --> id5 id9 -- production deployment --> id10","title":"Tools Utilized in Docker based Development"},{"location":"Container%20and%20Orchestration/About%20Docker/#step-1-docker-desktop-installation-on-windows","text":"when installing docker desktop, download the docker for windows. This might require installation of an older version of WSL to run it on the desktop. This is done from Microsoft website. WSL stands for windows subsystem for Linux. Using this you are able to run a Linux environment in windows. Manual installation steps for older versions of WSL | Microsoft Docs The commands for docker are available at Docker-Commands Once docker desktop is installed a simple docker based POC conducted can be performed following steps mentioned in Simple Docker Project These notes are based on my understanding of the topic. Some of the content and images refers to the online articles, learning courses I attended. I encourage you to lookup the links provided to develop your own understanding of it at deeper level. The intention of these notes is to serve as reference marker to other sites, online courses, books that I found useful while learning about it. If you would like to share any thoughts or give comments please feel free to reach me @ abhishekgupta86@gmail.com or @ www.linkedin.com/in/abhishekgupta86","title":"Step 1: Docker Desktop Installation on Windows"},{"location":"Container%20and%20Orchestration/CI%20with%20Travis/","text":"Travis based Continuous Integration \u00b6 These notes are based on the LinkedIn course - Docker for developers by Emmanuel Henri - Docker for developers (linkedin.com) Travis CI is a cloud based managed service that can be used for integrating it with a GitHub repos to continuously build and compile the repo. Similar to Travis there are many other CI platforms that can be used to achieve the same objective. example - Circle CI, Azure CI etc. Travis is a ruby based CI platform that gets triggered based on the .travis.yml file that must be present at the root of the project folder that you need to build To check for the file that has been used for this POC refer the code repository at Github - abhishekgupta-myrepo/travis-ci_poc (github.com) Travis platform gets trigged based on everytime a commit happens to the repository that has been configured with Travis for build. In order to use Travis you will need to create an account with Travis and will have to specify the repo in github that you would like to be configured. Process for Continuous Integration \u00b6 flowchart LR id1([develop new function]) id2([commit changes to repo]) id3([CI provider tests the build]) id4([deploy the code]) id1-->id2 id2--> id3 id3--> id4 id4--> id1 Docker fits well into this process where each time the build is triggered in Tavis it can compile and then create the docker image that can be pushed onto a remote repo. Using Docker in Builds - Travis CI (travis-ci.com) You can view the history of the build as well as view the current build that is in process. Travis provides you with console where you can view the logs as the build is in progress. The same .travis.yml can the be used to push the image that has been created into the remote repo such as docker hub. At the end of this CI pipeline a CD (Continuous Deployment) platform can then take up the image pushed into the remote repo to update the Kubernetes cluster based environment running. These notes are based on my understanding of the topic. Some of the content and images refers to the online articles, learning courses I attended. I encourage you to lookup the links provided to develop your own understanding of it at deeper level. The intention of these notes is to serve as reference marker to other sites, online courses, books that I found useful while learning about it. If you would like to share any thoughts or give comments please feel free to reach me @ abhishekgupta86@gmail.com or @ www.linkedin.com/in/abhishekgupta86","title":"Travis based Continuous Integration"},{"location":"Container%20and%20Orchestration/CI%20with%20Travis/#travis-based-continuous-integration","text":"These notes are based on the LinkedIn course - Docker for developers by Emmanuel Henri - Docker for developers (linkedin.com) Travis CI is a cloud based managed service that can be used for integrating it with a GitHub repos to continuously build and compile the repo. Similar to Travis there are many other CI platforms that can be used to achieve the same objective. example - Circle CI, Azure CI etc. Travis is a ruby based CI platform that gets triggered based on the .travis.yml file that must be present at the root of the project folder that you need to build To check for the file that has been used for this POC refer the code repository at Github - abhishekgupta-myrepo/travis-ci_poc (github.com) Travis platform gets trigged based on everytime a commit happens to the repository that has been configured with Travis for build. In order to use Travis you will need to create an account with Travis and will have to specify the repo in github that you would like to be configured.","title":"Travis based Continuous Integration"},{"location":"Container%20and%20Orchestration/CI%20with%20Travis/#process-for-continuous-integration","text":"flowchart LR id1([develop new function]) id2([commit changes to repo]) id3([CI provider tests the build]) id4([deploy the code]) id1-->id2 id2--> id3 id3--> id4 id4--> id1 Docker fits well into this process where each time the build is triggered in Tavis it can compile and then create the docker image that can be pushed onto a remote repo. Using Docker in Builds - Travis CI (travis-ci.com) You can view the history of the build as well as view the current build that is in process. Travis provides you with console where you can view the logs as the build is in progress. The same .travis.yml can the be used to push the image that has been created into the remote repo such as docker hub. At the end of this CI pipeline a CD (Continuous Deployment) platform can then take up the image pushed into the remote repo to update the Kubernetes cluster based environment running. These notes are based on my understanding of the topic. Some of the content and images refers to the online articles, learning courses I attended. I encourage you to lookup the links provided to develop your own understanding of it at deeper level. The intention of these notes is to serve as reference marker to other sites, online courses, books that I found useful while learning about it. If you would like to share any thoughts or give comments please feel free to reach me @ abhishekgupta86@gmail.com or @ www.linkedin.com/in/abhishekgupta86","title":"Process for Continuous Integration"},{"location":"Container%20and%20Orchestration/Docker%20Compose/","text":"Docker Compose \u00b6 Instructions below are based on following the directions provided during the course : Docker for developers by Emmanuel Henri - Docker for developers (linkedin.com) Follow the steps below to develop a full stack service comprising of multiple containers. The instructions for developing a full stack are written the docker compose .yml file. Look at the sample project developed available at the GitHub - abhishekgupta-myrepo/docker_poc (github.com) Once the docker compose file is written preform the following steps to create a service. Step 1 : Run the below command to create images for all the services that form part of the full stack and must be deployed as a single unit. PS C:\\Users\\abhis\\Projects\\docker_poc> docker-compose build [+] Building 22.1s (11/11) FINISHED => [internal] load build definition from Dockerfile 0.0s => => transferring dockerfile: 155B 0.0s => [internal] load .dockerignore 0.0s => => transferring context: 66B 0.0s => [internal] load metadata for docker.io/library/node:latest 1.7s => [auth] library/node:pull token for registry-1.docker.io 0.0s => [1/5] FROM docker.io/library/node@sha256:8a45c95c328809e7e10e8c9ed5bf8374620d62e52de1df7ef8e71a9596e 0.0s => [internal] load build context 0.3s => => transferring context: 16.08MB 0.3s => CACHED [2/5] WORKDIR /usr/src/app 0.0s => [3/5] COPY package*.json ./ 0.1s => [4/5] RUN npm install 18.3s => [5/5] COPY . . 0.1s => exporting to image 1.4s => => exporting layers 1.4s => => writing image sha256:3b656d9f2fa551b00589d89b61f563f07c33c7f8eb2a2ce191a7b6d2d6c14576 0.0s => => naming to docker.io/library/docker_poc-app 0.0s Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them Step 2 : Run the following command to spin up specific container to be created as part of the full stack. Note: Even if you bring up each container one at a time even then all the containers form part of the stack. Make sure to trigger the containers in the order so that the other container does not fail that is dependent upon the other container for successful boot. This is done by mentioning the specific container to kickoff Reason for specifying the -d and mentioning the container that needs to start first is that docker compose will start all the containers in parallel and will run into issues if the database is not up. PS C:\\Users\\abhis\\Projects\\docker_poc> docker-compose up -d mongo [+] Running 10/10 - mongo Pulled 19.8s - 675920708c8b Pull complete 5.8s - 6f9c8c301e0f Pull complete 5.9s - 73738965c4ce Pull complete 6.3s - 7fd6635b9ddf Pull complete 6.9s - 73a471eaa4ad Pull complete 7.0s - bcf274af89b0 Pull complete 7.1s - 04fc489f2a3b Pull complete 7.2s - 6eff8a505292 Pull complete 18.6s - a5ef4431fce7 Pull complete 18.6s [+] Running 2/2 - Network docker_poc_default Created 0.0s - Container mongo Started 1.1s PS C:\\Users\\abhis\\Projects\\docker_poc> Step 3 : Run the below command to look into the logs of specific container. This is not a core part of the sequence of instructions to spin up this stack and is just to validate the state of the containers. PS C:\\Users\\abhis\\Projects\\docker_poc> docker logs e74cd {\"t\":{\"$date\":\"2022-09-15T20:59:49.287+00:00\"},\"s\":\"I\", \"c\":\"NETWORK\", \"id\":4915701, \"ctx\":\"-\",\"msg\":\"Initialized wire specification\",\"attr\":{\"spec\":{\"incomingExternalClient\":{\"minWireVersion\":0,\"maxWireVersion\":17},\"incomingInternalClient\":{\"minWireVersion\":0,\"maxWireVersion\":17},\"outgoing\":{\"minWireVersion\":6,\"maxWireVersion\":17},\"isInternalClient\":true}}} Step 4 : Run the below command to stop all the containers that form part of a stack built through the docker compose. Stopping all the containers can be done by a single command Make sure to stop the docker compose stop to stop the service else if you stop the containers individually they will spin up again. PS C:\\Users\\abhis\\Projects\\docker_poc> docker-compose stop [+] Running 2/2 - Container app Stopped 1.0s - Container mongo Stopped Once the docker compose is ready for deployment it can be used for deployment through container orchestration tools such as - Docker Swarm: look at the notes @ Docker-Swarm . This is native to Docker and comes installed with docker desktop. Kubernetes: look at the notes @ Kubernetes . It is the current industry standard. Instead of setting up your own Kubernetes cluster most common practice is to use it as a managed services through cloud providers such as GCP or AWS. Mesos These notes are based on my understanding of the topic. Some of the content and images refers to the online articles, learning courses I attended. I encourage you to lookup the links provided to develop your own understanding of it at deeper level. The intention of these notes is to serve as reference marker to other sites, online courses, books that I found useful while learning about it. If you would like to share any thoughts or give comments please feel free to reach me @ abhishekgupta86@gmail.com or @ www.linkedin.com/in/abhishekgupta86","title":"Docker Compose"},{"location":"Container%20and%20Orchestration/Docker%20Compose/#docker-compose","text":"Instructions below are based on following the directions provided during the course : Docker for developers by Emmanuel Henri - Docker for developers (linkedin.com) Follow the steps below to develop a full stack service comprising of multiple containers. The instructions for developing a full stack are written the docker compose .yml file. Look at the sample project developed available at the GitHub - abhishekgupta-myrepo/docker_poc (github.com) Once the docker compose file is written preform the following steps to create a service. Step 1 : Run the below command to create images for all the services that form part of the full stack and must be deployed as a single unit. PS C:\\Users\\abhis\\Projects\\docker_poc> docker-compose build [+] Building 22.1s (11/11) FINISHED => [internal] load build definition from Dockerfile 0.0s => => transferring dockerfile: 155B 0.0s => [internal] load .dockerignore 0.0s => => transferring context: 66B 0.0s => [internal] load metadata for docker.io/library/node:latest 1.7s => [auth] library/node:pull token for registry-1.docker.io 0.0s => [1/5] FROM docker.io/library/node@sha256:8a45c95c328809e7e10e8c9ed5bf8374620d62e52de1df7ef8e71a9596e 0.0s => [internal] load build context 0.3s => => transferring context: 16.08MB 0.3s => CACHED [2/5] WORKDIR /usr/src/app 0.0s => [3/5] COPY package*.json ./ 0.1s => [4/5] RUN npm install 18.3s => [5/5] COPY . . 0.1s => exporting to image 1.4s => => exporting layers 1.4s => => writing image sha256:3b656d9f2fa551b00589d89b61f563f07c33c7f8eb2a2ce191a7b6d2d6c14576 0.0s => => naming to docker.io/library/docker_poc-app 0.0s Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them Step 2 : Run the following command to spin up specific container to be created as part of the full stack. Note: Even if you bring up each container one at a time even then all the containers form part of the stack. Make sure to trigger the containers in the order so that the other container does not fail that is dependent upon the other container for successful boot. This is done by mentioning the specific container to kickoff Reason for specifying the -d and mentioning the container that needs to start first is that docker compose will start all the containers in parallel and will run into issues if the database is not up. PS C:\\Users\\abhis\\Projects\\docker_poc> docker-compose up -d mongo [+] Running 10/10 - mongo Pulled 19.8s - 675920708c8b Pull complete 5.8s - 6f9c8c301e0f Pull complete 5.9s - 73738965c4ce Pull complete 6.3s - 7fd6635b9ddf Pull complete 6.9s - 73a471eaa4ad Pull complete 7.0s - bcf274af89b0 Pull complete 7.1s - 04fc489f2a3b Pull complete 7.2s - 6eff8a505292 Pull complete 18.6s - a5ef4431fce7 Pull complete 18.6s [+] Running 2/2 - Network docker_poc_default Created 0.0s - Container mongo Started 1.1s PS C:\\Users\\abhis\\Projects\\docker_poc> Step 3 : Run the below command to look into the logs of specific container. This is not a core part of the sequence of instructions to spin up this stack and is just to validate the state of the containers. PS C:\\Users\\abhis\\Projects\\docker_poc> docker logs e74cd {\"t\":{\"$date\":\"2022-09-15T20:59:49.287+00:00\"},\"s\":\"I\", \"c\":\"NETWORK\", \"id\":4915701, \"ctx\":\"-\",\"msg\":\"Initialized wire specification\",\"attr\":{\"spec\":{\"incomingExternalClient\":{\"minWireVersion\":0,\"maxWireVersion\":17},\"incomingInternalClient\":{\"minWireVersion\":0,\"maxWireVersion\":17},\"outgoing\":{\"minWireVersion\":6,\"maxWireVersion\":17},\"isInternalClient\":true}}} Step 4 : Run the below command to stop all the containers that form part of a stack built through the docker compose. Stopping all the containers can be done by a single command Make sure to stop the docker compose stop to stop the service else if you stop the containers individually they will spin up again. PS C:\\Users\\abhis\\Projects\\docker_poc> docker-compose stop [+] Running 2/2 - Container app Stopped 1.0s - Container mongo Stopped Once the docker compose is ready for deployment it can be used for deployment through container orchestration tools such as - Docker Swarm: look at the notes @ Docker-Swarm . This is native to Docker and comes installed with docker desktop. Kubernetes: look at the notes @ Kubernetes . It is the current industry standard. Instead of setting up your own Kubernetes cluster most common practice is to use it as a managed services through cloud providers such as GCP or AWS. Mesos These notes are based on my understanding of the topic. Some of the content and images refers to the online articles, learning courses I attended. I encourage you to lookup the links provided to develop your own understanding of it at deeper level. The intention of these notes is to serve as reference marker to other sites, online courses, books that I found useful while learning about it. If you would like to share any thoughts or give comments please feel free to reach me @ abhishekgupta86@gmail.com or @ www.linkedin.com/in/abhishekgupta86","title":"Docker Compose"},{"location":"Container%20and%20Orchestration/Docker-Commands/","text":"Docker Commands Cheat sheet \u00b6 Command below provides all the options available with Docker. All the docker commands start wit the word docker AGupta@vm2 MINGW64 ~ $ docker Usage: docker [OPTIONS] COMMAND Options: --config string Location of client config files (default \"C:UsersAGupta.docker\") -c, --context string Name of the context to use to connect to the daemon (overrides DOCKER_HOST env var and default context set with \"docker context use\") -D, --debug Enable debug mode -H, --host list Daemon socket(s) to connect to -l, --log-level string Set the logging level (\"debug\"|\"info\"|\"warn\"|\"error\"|\"fatal\") (default \"info\") --tls Use TLS; implied by --tlsverify --tlscacert string Trust certs signed only by this CA (default \"C:UsersAGupta.dockerca.pem\") --tlscert string Path to TLS certificate file (default \"C:UsersAGupta.dockercert.pem\") --tlskey string Path to TLS key file (default \"C:UsersAGupta.dockerkey.pem\") --tlsverify Use TLS and verify the remote -v, --version Print version information and quit Management Commands Command Description builder Manage builds config Manage Docker configs container Manage containers context Manage contexts image Manage images network Manage networks node Manage Swarm nodes plugin Manage plugins secret Manage Docker secrets service Manage services stack Manage Docker stacks swarm Manage Swarm system Manage Docker trust Manage trust on Docker images volume Manage volumes Commands Command Description attach Attach local standard input, output, and error streams to a running container build Build an image from a Dockerfile commit Create a new image from a container's changes cp Copy files/folders between a container and the local filesystem create Create a new container deploy Deploy a new stack or update an existing stack diff Inspect changes to files or directories on a container's filesystem events Get real time events from the server exec Run a command in a running container export Export a container's filesystem as a tar archive history Show the history of an image images List images import Import the contents from a tarball to create a filesystem image info Display system-wide information inspect Return low-level information on Docker objects kill Kill one or more running containers load Load an image from a tar archive or STDIN login Log in to a Docker registry logout Log out from a Docker registry logs Fetch the logs of a container pause Pause all processes within one or more containers port List port mappings or a specific mapping for the container ps List containers pull Pull an image or a repository from a registry push Push an image or a repository to a registry rename Rename a container restart Restart one or more containers rm Remove one or more containers rmi Remove one or more images run Run a command in a new container save Save one or more images to a tar archive (streamed to STDOUT by default) search Search the Docker Hub for images start Start one or more stopped containers stats Display a live stream of container(s) resource usage statistics stop Stop one or more running containers tag Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE top Display the running processes of a container unpause Unpause all processes within one or more containers update Update configuration of one or more containers version Show the Docker version information wait Block until one or more containers stop, then print their exit codes Any of the commands described above can be used by following the syntax below - Run 'docker COMMAND --help' for more information on a command. To understand how to run a simple Docker project look at Simple Docker Project","title":"Docker Commands Cheat sheet"},{"location":"Container%20and%20Orchestration/Docker-Commands/#docker-commands-cheat-sheet","text":"Command below provides all the options available with Docker. All the docker commands start wit the word docker AGupta@vm2 MINGW64 ~ $ docker Usage: docker [OPTIONS] COMMAND Options: --config string Location of client config files (default \"C:UsersAGupta.docker\") -c, --context string Name of the context to use to connect to the daemon (overrides DOCKER_HOST env var and default context set with \"docker context use\") -D, --debug Enable debug mode -H, --host list Daemon socket(s) to connect to -l, --log-level string Set the logging level (\"debug\"|\"info\"|\"warn\"|\"error\"|\"fatal\") (default \"info\") --tls Use TLS; implied by --tlsverify --tlscacert string Trust certs signed only by this CA (default \"C:UsersAGupta.dockerca.pem\") --tlscert string Path to TLS certificate file (default \"C:UsersAGupta.dockercert.pem\") --tlskey string Path to TLS key file (default \"C:UsersAGupta.dockerkey.pem\") --tlsverify Use TLS and verify the remote -v, --version Print version information and quit Management Commands Command Description builder Manage builds config Manage Docker configs container Manage containers context Manage contexts image Manage images network Manage networks node Manage Swarm nodes plugin Manage plugins secret Manage Docker secrets service Manage services stack Manage Docker stacks swarm Manage Swarm system Manage Docker trust Manage trust on Docker images volume Manage volumes Commands Command Description attach Attach local standard input, output, and error streams to a running container build Build an image from a Dockerfile commit Create a new image from a container's changes cp Copy files/folders between a container and the local filesystem create Create a new container deploy Deploy a new stack or update an existing stack diff Inspect changes to files or directories on a container's filesystem events Get real time events from the server exec Run a command in a running container export Export a container's filesystem as a tar archive history Show the history of an image images List images import Import the contents from a tarball to create a filesystem image info Display system-wide information inspect Return low-level information on Docker objects kill Kill one or more running containers load Load an image from a tar archive or STDIN login Log in to a Docker registry logout Log out from a Docker registry logs Fetch the logs of a container pause Pause all processes within one or more containers port List port mappings or a specific mapping for the container ps List containers pull Pull an image or a repository from a registry push Push an image or a repository to a registry rename Rename a container restart Restart one or more containers rm Remove one or more containers rmi Remove one or more images run Run a command in a new container save Save one or more images to a tar archive (streamed to STDOUT by default) search Search the Docker Hub for images start Start one or more stopped containers stats Display a live stream of container(s) resource usage statistics stop Stop one or more running containers tag Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE top Display the running processes of a container unpause Unpause all processes within one or more containers update Update configuration of one or more containers version Show the Docker version information wait Block until one or more containers stop, then print their exit codes Any of the commands described above can be used by following the syntax below - Run 'docker COMMAND --help' for more information on a command. To understand how to run a simple Docker project look at Simple Docker Project","title":"Docker Commands Cheat sheet"},{"location":"Container%20and%20Orchestration/Docker-Swarm/","text":"Introduction to Swarm \u00b6 Instructions below are based on following the directions provided during the course : Docker for developers by Emmanuel Henri - Docker for developers (linkedin.com) Docker Swarm is a tool to manage cluster of nodes. Each node runs multiple containers. A node is a physical or a virtual machine on which multiple containers are run. The swarm then provides the ability to load balance the nodes and manage multiple nodes within the clusters. flowchart TB subgraph Node one a1-->a2 end subgraph Node two b1-->b2 end subgraph Node three c1-->c2 end Docker swarm works on the principle of master and slave model. The master node has the swarm that manages the worked nodes on which the containers are deployed. On a local machine you can setup a single node swarm where the same node can be used for running the containers and the swarm to manage it. On the servers each node that needs to be joined with the master must be joined and must have the docker installed. Once it is joined to the master node Swarm it then takes care of deploying the containers on the nodes. Docker swarm comes installed with docker desktop. Follow the below commands to run the docker swarm locally. This requires the == docker compose .yml== file that you can refer from the GitHub project @ - abhishekgupta-myrepo/docker_poc (github.com) Step 1 : Run the below command in PowerShell from Visual Studio code PS C:\\Users\\abhis\\Projects\\docker_poc> docker swarm init Swarm initialized: current node (82w06t5v7kdki2pncbb93ivvy) is now a manager. To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-4mou11qdvv4zggmgdhrtq33kmphvhrxtqe0pqq99ow9foxbyp5-e7868tp81uaun9qtkcrel1n6h 192.168.65.3:2377 To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. You need to ssh into the machine that you want to add to the swarm where you need to run the command docker swarm join --token SWMTKN-1-4mou11qdvv4zggmgdhrtq33kmphvhrxtqe0pqq99ow9foxbyp5-e7868tp81uaun9qtkcrel1n6h 192.168.65.3:2377 Step 2 : Check for the information about the docker setup that will also provide details about the docker swam that has been initialized. PS C:\\Users\\abhis\\Projects\\docker_poc> docker info Client: Context: default Debug Mode: false Plugins: buildx: Docker Buildx (Docker Inc., v0.9.1) compose: Docker Compose (Docker Inc., v2.10.2) extension: Manages Docker extensions (Docker Inc., v0.2.9) sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc., 0.6.0) scan: Docker Scan (Docker Inc., v0.19.0) Server: Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 20.10.17 Storage Driver: overlay2 Backing Filesystem: extfs Supports d_type: true Native Overlay Diff: true userxattr: false Logging Driver: json-file Cgroup Driver: cgroupfs Cgroup Version: 1 Plugins: Volume: local Network: bridge host ipvlan macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog Swarm: active NodeID: 82w06t5v7kdki2pncbb93ivvy Is Manager: true ClusterID: wd8zdd14dyeflloa4jzdpdaoo Managers: 1 Nodes: 1 Default Address Pool: 10.0.0.0/8 SubnetSize: 24 Data Path Port: 4789 Orchestration: Task History Retention Limit: 5 Raft: Snapshot Interval: 10000 Number of Old Snapshots to Retain: 0 Heartbeat Tick: 1 Election Tick: 10 Dispatcher: Heartbeat Period: 5 seconds CA Configuration: Expiry Duration: 3 months Force Rotate: 0 Autolock Managers: false Root Rotation In Progress: false Node Address: 192.168.65.3 Manager Addresses: 192.168.65.3:2377 Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc Default Runtime: runc Init Binary: docker-init containerd version: 9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6 runc version: v1.1.4-0-g5fd4c4d init version: de40ad0 Security Options: seccomp Profile: default No Proxy: hubproxy.docker.internal Registry: https://index.docker.io/v1/ Labels: Experimental: false Insecure Registries: hubproxy.docker.internal:5000 127.0.0.0/8 Live Restore Enabled: false Step 3 : Look at the node that has been created - PS C:\\Users\\abhis\\Projects\\docker_poc> docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION 82w06t5v7kdki2pncbb93ivvy * docker-desktop Ready Active Leader 20.10.17 PS C:\\Users\\abhis\\Projects\\docker_poc> Step 4 : Run the below command to deploy the stack on docker swarm. For this command to run make sure that the image name is mentioned in the docker-compose file and the version of the docker-compose.yml is 3.0 PS C:\\Users\\abhis\\Projects\\docker_poc> docker stack deploy -c docker-compose.yml STACK Ignoring unsupported options: build, links, restart Ignoring deprecated options: container_name: Setting the container name is not supported. expose: Exposing ports is unnecessary - services on the same network can access each other's containers on any port. Updating service STACK_client (id: jaojw6zb20dihl7wgwkweu79c) image clientapp:latest could not be accessed on a registry to record its digest. Each node will access clientapp:latest independently, possibly leading to different nodes running different versions of the image. Creating service STACK_app Creating service STACK_mongo PS C:\\Users\\abhis\\Projects\\docker_poc> Step 5 : Validate that the docker containers are up and running. If you check all the containers will be instantiated as part of the stack that you described in the docker compose .yml and will be prefixed with the name that you provided to the stack at the time of docker stack deploy command PS C:\\Users\\abhis\\Projects\\docker_poc> docker service ls ID NAME MODE REPLICAS IMAGE PORTS 303uyn5tfhsg STACK_app replicated 1/1 backendapp:latest *:4000->4000/tcp jaojw6zb20di STACK_client replicated 1/1 clientapp:latest *:3000->3000/tcp nujukyytj1jl STACK_mongo replicated 1/1 mongo:latest *:27017->27017/tcp PS C:\\Users\\abhis\\Projects\\docker_poc> Step 6 : Stop the docker swarm based services using the command below - Stopping the stack service is important else it will keep running. -PS C:\\Users\\abhis\\Projects\\docker_poc> docker stack rm STACK Removing service STACK_app Removing service STACK_client Removing service STACK_mongo Removing network STACK_default To develop a CI pipeline from Github project using Travis CI app refer to the notes on CI with Travis - This takes it to full circle where once the code is developed, tested locally, and committed to the git repo in Github a continuous integration tool such as Travis can be used for continuous build that gets triggered for each commit to compile the code and push the image into a remote repository such as docker hub. These notes are based on my understanding of the topic. Some of the content and images refers to the online articles, learning courses I attended. I encourage you to lookup the links provided to develop your own understanding of it at deeper level. The intention of these notes is to serve as reference marker to other sites, online courses, books that I found useful while learning about it. If you would like to share any thoughts or give comments please feel free to reach me @ abhishekgupta86@gmail.com or @ www.linkedin.com/in/abhishekgupta86","title":"Introduction to Swarm"},{"location":"Container%20and%20Orchestration/Docker-Swarm/#introduction-to-swarm","text":"Instructions below are based on following the directions provided during the course : Docker for developers by Emmanuel Henri - Docker for developers (linkedin.com) Docker Swarm is a tool to manage cluster of nodes. Each node runs multiple containers. A node is a physical or a virtual machine on which multiple containers are run. The swarm then provides the ability to load balance the nodes and manage multiple nodes within the clusters. flowchart TB subgraph Node one a1-->a2 end subgraph Node two b1-->b2 end subgraph Node three c1-->c2 end Docker swarm works on the principle of master and slave model. The master node has the swarm that manages the worked nodes on which the containers are deployed. On a local machine you can setup a single node swarm where the same node can be used for running the containers and the swarm to manage it. On the servers each node that needs to be joined with the master must be joined and must have the docker installed. Once it is joined to the master node Swarm it then takes care of deploying the containers on the nodes. Docker swarm comes installed with docker desktop. Follow the below commands to run the docker swarm locally. This requires the == docker compose .yml== file that you can refer from the GitHub project @ - abhishekgupta-myrepo/docker_poc (github.com) Step 1 : Run the below command in PowerShell from Visual Studio code PS C:\\Users\\abhis\\Projects\\docker_poc> docker swarm init Swarm initialized: current node (82w06t5v7kdki2pncbb93ivvy) is now a manager. To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-4mou11qdvv4zggmgdhrtq33kmphvhrxtqe0pqq99ow9foxbyp5-e7868tp81uaun9qtkcrel1n6h 192.168.65.3:2377 To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. You need to ssh into the machine that you want to add to the swarm where you need to run the command docker swarm join --token SWMTKN-1-4mou11qdvv4zggmgdhrtq33kmphvhrxtqe0pqq99ow9foxbyp5-e7868tp81uaun9qtkcrel1n6h 192.168.65.3:2377 Step 2 : Check for the information about the docker setup that will also provide details about the docker swam that has been initialized. PS C:\\Users\\abhis\\Projects\\docker_poc> docker info Client: Context: default Debug Mode: false Plugins: buildx: Docker Buildx (Docker Inc., v0.9.1) compose: Docker Compose (Docker Inc., v2.10.2) extension: Manages Docker extensions (Docker Inc., v0.2.9) sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc., 0.6.0) scan: Docker Scan (Docker Inc., v0.19.0) Server: Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 20.10.17 Storage Driver: overlay2 Backing Filesystem: extfs Supports d_type: true Native Overlay Diff: true userxattr: false Logging Driver: json-file Cgroup Driver: cgroupfs Cgroup Version: 1 Plugins: Volume: local Network: bridge host ipvlan macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog Swarm: active NodeID: 82w06t5v7kdki2pncbb93ivvy Is Manager: true ClusterID: wd8zdd14dyeflloa4jzdpdaoo Managers: 1 Nodes: 1 Default Address Pool: 10.0.0.0/8 SubnetSize: 24 Data Path Port: 4789 Orchestration: Task History Retention Limit: 5 Raft: Snapshot Interval: 10000 Number of Old Snapshots to Retain: 0 Heartbeat Tick: 1 Election Tick: 10 Dispatcher: Heartbeat Period: 5 seconds CA Configuration: Expiry Duration: 3 months Force Rotate: 0 Autolock Managers: false Root Rotation In Progress: false Node Address: 192.168.65.3 Manager Addresses: 192.168.65.3:2377 Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc Default Runtime: runc Init Binary: docker-init containerd version: 9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6 runc version: v1.1.4-0-g5fd4c4d init version: de40ad0 Security Options: seccomp Profile: default No Proxy: hubproxy.docker.internal Registry: https://index.docker.io/v1/ Labels: Experimental: false Insecure Registries: hubproxy.docker.internal:5000 127.0.0.0/8 Live Restore Enabled: false Step 3 : Look at the node that has been created - PS C:\\Users\\abhis\\Projects\\docker_poc> docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION 82w06t5v7kdki2pncbb93ivvy * docker-desktop Ready Active Leader 20.10.17 PS C:\\Users\\abhis\\Projects\\docker_poc> Step 4 : Run the below command to deploy the stack on docker swarm. For this command to run make sure that the image name is mentioned in the docker-compose file and the version of the docker-compose.yml is 3.0 PS C:\\Users\\abhis\\Projects\\docker_poc> docker stack deploy -c docker-compose.yml STACK Ignoring unsupported options: build, links, restart Ignoring deprecated options: container_name: Setting the container name is not supported. expose: Exposing ports is unnecessary - services on the same network can access each other's containers on any port. Updating service STACK_client (id: jaojw6zb20dihl7wgwkweu79c) image clientapp:latest could not be accessed on a registry to record its digest. Each node will access clientapp:latest independently, possibly leading to different nodes running different versions of the image. Creating service STACK_app Creating service STACK_mongo PS C:\\Users\\abhis\\Projects\\docker_poc> Step 5 : Validate that the docker containers are up and running. If you check all the containers will be instantiated as part of the stack that you described in the docker compose .yml and will be prefixed with the name that you provided to the stack at the time of docker stack deploy command PS C:\\Users\\abhis\\Projects\\docker_poc> docker service ls ID NAME MODE REPLICAS IMAGE PORTS 303uyn5tfhsg STACK_app replicated 1/1 backendapp:latest *:4000->4000/tcp jaojw6zb20di STACK_client replicated 1/1 clientapp:latest *:3000->3000/tcp nujukyytj1jl STACK_mongo replicated 1/1 mongo:latest *:27017->27017/tcp PS C:\\Users\\abhis\\Projects\\docker_poc> Step 6 : Stop the docker swarm based services using the command below - Stopping the stack service is important else it will keep running. -PS C:\\Users\\abhis\\Projects\\docker_poc> docker stack rm STACK Removing service STACK_app Removing service STACK_client Removing service STACK_mongo Removing network STACK_default To develop a CI pipeline from Github project using Travis CI app refer to the notes on CI with Travis - This takes it to full circle where once the code is developed, tested locally, and committed to the git repo in Github a continuous integration tool such as Travis can be used for continuous build that gets triggered for each commit to compile the code and push the image into a remote repository such as docker hub. These notes are based on my understanding of the topic. Some of the content and images refers to the online articles, learning courses I attended. I encourage you to lookup the links provided to develop your own understanding of it at deeper level. The intention of these notes is to serve as reference marker to other sites, online courses, books that I found useful while learning about it. If you would like to share any thoughts or give comments please feel free to reach me @ abhishekgupta86@gmail.com or @ www.linkedin.com/in/abhishekgupta86","title":"Introduction to Swarm"},{"location":"Container%20and%20Orchestration/Kubernetes/","text":"Simple Kubernetes Project \u00b6 A Kubernetes cluster can be deployed and tested locally following the steps mentioned below - This simplicity must not be confused with the actual complexity and the depth of the functionality that Kubernetes provides for running production clusters. Kubernetes is currently far and wide an industry standard. This is generally used for running 10000+ nodes clusters. It is now an open source project originally developed by google. Mesos is the nearest competitor that is used by many giants for running their production environment. Step 1 : Install Kubectl from the website for the local deployment of the cluster Getting started | Kubernetes Step 2 : Install Kind or Minikube which is used for running deployments in local or in development environment. Both the options are provided on the Kubernetes website. minikube start | minikube (k8s.io) Run the command minikube start . Note this must be run from the command prompt Ensure that the .exe file that you downloaded for the minikube is set in the PATH environment variable. Ensure that prior to starting the minikube cluster you have installed kubectl The steps to install kubectl [which the command line for Kubernetes] are similar to installing the minikube. i.e. once the .exe file for kubectl has been downloaded setup the PATH variable for kubectl.exe file. Step 3 : Run this command to get information about the cluster kubectl cluster-info Step 4 : Run this command to deploy the container image in the minikube cluster. ==kubectl create deployment nodeapplication2 -- image=node == Note: Minikube pulls the image provided either from Dockerhub by default or from the specified repo that can be mentioned as the full alternate url to the image. Unlike docker swarm that pulls the images and the services stack from the docker compose .yml minikube pulls images directly that have been pushed into the repo. So in case you need test the local cluster deployment you will need to push the image to Docker hub repo from where it will be pulled. Step 5 : Run this command to check the deployment kubectl get deployments To develop a CI pipeline from Github project using Travis CI app refer to the notes on CI with Travis - This takes it to full circle where once the code is developed, tested locally, and committed to the git repo in Github a continuous integration tool such as Travis can be used for continuous build that gets triggered for each commit to compile the code and push the image into a remote repository such as docker hub.","title":"Simple Kubernetes Project"},{"location":"Container%20and%20Orchestration/Kubernetes/#simple-kubernetes-project","text":"A Kubernetes cluster can be deployed and tested locally following the steps mentioned below - This simplicity must not be confused with the actual complexity and the depth of the functionality that Kubernetes provides for running production clusters. Kubernetes is currently far and wide an industry standard. This is generally used for running 10000+ nodes clusters. It is now an open source project originally developed by google. Mesos is the nearest competitor that is used by many giants for running their production environment. Step 1 : Install Kubectl from the website for the local deployment of the cluster Getting started | Kubernetes Step 2 : Install Kind or Minikube which is used for running deployments in local or in development environment. Both the options are provided on the Kubernetes website. minikube start | minikube (k8s.io) Run the command minikube start . Note this must be run from the command prompt Ensure that the .exe file that you downloaded for the minikube is set in the PATH environment variable. Ensure that prior to starting the minikube cluster you have installed kubectl The steps to install kubectl [which the command line for Kubernetes] are similar to installing the minikube. i.e. once the .exe file for kubectl has been downloaded setup the PATH variable for kubectl.exe file. Step 3 : Run this command to get information about the cluster kubectl cluster-info Step 4 : Run this command to deploy the container image in the minikube cluster. ==kubectl create deployment nodeapplication2 -- image=node == Note: Minikube pulls the image provided either from Dockerhub by default or from the specified repo that can be mentioned as the full alternate url to the image. Unlike docker swarm that pulls the images and the services stack from the docker compose .yml minikube pulls images directly that have been pushed into the repo. So in case you need test the local cluster deployment you will need to push the image to Docker hub repo from where it will be pulled. Step 5 : Run this command to check the deployment kubectl get deployments To develop a CI pipeline from Github project using Travis CI app refer to the notes on CI with Travis - This takes it to full circle where once the code is developed, tested locally, and committed to the git repo in Github a continuous integration tool such as Travis can be used for continuous build that gets triggered for each commit to compile the code and push the image into a remote repository such as docker hub.","title":"Simple Kubernetes Project"},{"location":"Container%20and%20Orchestration/Simple%20Docker%20Project/","text":"Compile simple project in docker \u00b6 Instructions below are based on following the directions provided during the course : Docker for developers by Emmanuel Henri - Docker for developers (linkedin.com) Follow the below steps to setup a simple backend container based docker project. Step 1 - Create the Dockerfile. Refer to the GitHub project - abhishekgupta-myrepo/docker_poc (github.com) The backend project in within the /app folder in the git project. To run and compile the docker image, go to the app folder in the Visual studio code PowerShell terminal to type in the commands below - Before you run the commands make sure you have the docker desktop installed. Follow the steps mentioned in About Docker Make sure you have installed the VS code extension as it helps in suggesting the commands when writing Dockerfile. All the commands are uppercase Step 2 - Run the docker command to build the image - PS C:\\Users\\abhis\\Projects\\docker_poc> docker --version Docker version 20.10.17, build 100c701 PS C:\\Users\\abhis\\Projects\\docker_poc> docker build -t simplebackend . [+] Building 51.1s (11/11) FINISHED => [internal] load build definition from Dockerfile 0.1s => => transferring dockerfile: 155B 0.0s => [internal] load .dockerignore 0.0s => => transferring context: 66B 0.0s => [internal] load metadata for docker.io/library/node:latest 2.1s => [auth] library/node:pull token for registry-1.docker.io 0.0s => [internal] load build context 0.1s => => transferring context: 30.39kB 0.0s => [1/5] FROM docker.io/library/node@sha256:8a45c95c328809e7e10e8c9ed5bf8374620d62e52de1df7ef8e71a9596ec8676 30.5s => => resolve => => extracting sha256:8471b75885efc7790a16be5328e3b368567b76a60fc3feabd6869c15e175ee05 4.7s ...... Multiple lines of progress...... => => extracting sha256:b51256989aaaa6444b004b6f1dc3753c7c5a0f2132187622ee395e1327c36061 0.0s => [2/5] WORKDIR /usr/src/app 0.5s => [3/5] COPY package*.json ./ 0.0s => [4/5] RUN npm install 16.4s => [5/5] COPY . . 0.0s => exporting to image 1.4s => => exporting layers 1.4s => => writing image sha256:27a88271fc4f9634fe0a96e8e79ad533303f454f01966daa78fa5562931c4760 0.0s => => naming to docker.io/library/simplebackend 0.0s Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them PS C:\\Users\\abhis\\Projects\\docker_poc> Step 3 - Run the following docker command to check the image built locally- PS C:\\Users\\abhis\\Projects\\docker_poc> docker images REPOSITORY TAG IMAGE ID CREATED SIZE simplebackend latest 27a88271fc4f 6 minutes ago 1.05GB alpine/git latest 692618a0d74d 2 weeks ago 43.4MB Step 4 - Run the following docker command to instantiate container based on the image that you just created in the previous step- Mention about the port using -p and bind the container port to the local machine/VM port as show in the command below. Refer to the image using the tag that you assigned during the build process. PS C:\\Users\\abhis\\Projects\\docker_poc> docker run -p 4000:4000 simplebackend > backend@1.0.0 start > nodemon --exec babel-node index.js [nodemon] 2.0.19 [nodemon] to restart at any time, enter `rs` [nodemon] watching path(s): *.* [nodemon] watching extensions: js,mjs,json [nodemon] starting `babel-node index.js` Your server is running on PORT: 4000 Step 5 - Run the following docker command to check if the container is up and running as a service The container running can also be checked from the docker desktop as well as the number of images that have been created. Make sure that the container that you are running you shut down using the stop command Also, keep a check on the number of images that have been created. These images general run into 1-2 GBs and will very quickly eat up your local disk space. PS C:\\Users\\abhis\\Projects\\docker_poc> docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES cd637f86b4a8 simplebackend \"docker-entrypoint.s\u2026\" 33 seconds ago Up 32 seconds 0.0.0.0:4000->4000/tcp great_heyrovsky PS C:\\Users\\abhis\\Projects\\docker_poc> PS C:\\Users\\abhis\\Projects\\docker_poc> docker stop cd637 cd637 For running a full stack that comprise of frontend , backend, and a database consists of spinning up multiple containers that often form a stack. These containers can be combined to create a service that can be built suing docker compose To look at how to setup a multi-container based full stack service look at the notes Docker Compose To develop a CI pipeline from Github project using Travis CI app refer to the notes on CI with Travis This takes it to full circle where once the code is developed, tested locally, and committed to the git repo in Github a continuous integration tool such as Travis can be used for continuous build that gets triggered for each commit to compile the code and push the image into a remote repository such as docker hub. These notes are based on my understanding of the topic. Some of the content and images refers to the online articles, learning courses I attended. I encourage you to lookup the links provided to develop your own understanding of it at deeper level. The intention of these notes is to serve as reference marker to other sites, online courses, books that I found useful while learning about it. If you would like to share any thoughts or give comments please feel free to reach me @ abhishekgupta86@gmail.com or @ www.linkedin.com/in/abhishekgupta86","title":"Compile simple project in docker"},{"location":"Container%20and%20Orchestration/Simple%20Docker%20Project/#compile-simple-project-in-docker","text":"Instructions below are based on following the directions provided during the course : Docker for developers by Emmanuel Henri - Docker for developers (linkedin.com) Follow the below steps to setup a simple backend container based docker project. Step 1 - Create the Dockerfile. Refer to the GitHub project - abhishekgupta-myrepo/docker_poc (github.com) The backend project in within the /app folder in the git project. To run and compile the docker image, go to the app folder in the Visual studio code PowerShell terminal to type in the commands below - Before you run the commands make sure you have the docker desktop installed. Follow the steps mentioned in About Docker Make sure you have installed the VS code extension as it helps in suggesting the commands when writing Dockerfile. All the commands are uppercase Step 2 - Run the docker command to build the image - PS C:\\Users\\abhis\\Projects\\docker_poc> docker --version Docker version 20.10.17, build 100c701 PS C:\\Users\\abhis\\Projects\\docker_poc> docker build -t simplebackend . [+] Building 51.1s (11/11) FINISHED => [internal] load build definition from Dockerfile 0.1s => => transferring dockerfile: 155B 0.0s => [internal] load .dockerignore 0.0s => => transferring context: 66B 0.0s => [internal] load metadata for docker.io/library/node:latest 2.1s => [auth] library/node:pull token for registry-1.docker.io 0.0s => [internal] load build context 0.1s => => transferring context: 30.39kB 0.0s => [1/5] FROM docker.io/library/node@sha256:8a45c95c328809e7e10e8c9ed5bf8374620d62e52de1df7ef8e71a9596ec8676 30.5s => => resolve => => extracting sha256:8471b75885efc7790a16be5328e3b368567b76a60fc3feabd6869c15e175ee05 4.7s ...... Multiple lines of progress...... => => extracting sha256:b51256989aaaa6444b004b6f1dc3753c7c5a0f2132187622ee395e1327c36061 0.0s => [2/5] WORKDIR /usr/src/app 0.5s => [3/5] COPY package*.json ./ 0.0s => [4/5] RUN npm install 16.4s => [5/5] COPY . . 0.0s => exporting to image 1.4s => => exporting layers 1.4s => => writing image sha256:27a88271fc4f9634fe0a96e8e79ad533303f454f01966daa78fa5562931c4760 0.0s => => naming to docker.io/library/simplebackend 0.0s Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them PS C:\\Users\\abhis\\Projects\\docker_poc> Step 3 - Run the following docker command to check the image built locally- PS C:\\Users\\abhis\\Projects\\docker_poc> docker images REPOSITORY TAG IMAGE ID CREATED SIZE simplebackend latest 27a88271fc4f 6 minutes ago 1.05GB alpine/git latest 692618a0d74d 2 weeks ago 43.4MB Step 4 - Run the following docker command to instantiate container based on the image that you just created in the previous step- Mention about the port using -p and bind the container port to the local machine/VM port as show in the command below. Refer to the image using the tag that you assigned during the build process. PS C:\\Users\\abhis\\Projects\\docker_poc> docker run -p 4000:4000 simplebackend > backend@1.0.0 start > nodemon --exec babel-node index.js [nodemon] 2.0.19 [nodemon] to restart at any time, enter `rs` [nodemon] watching path(s): *.* [nodemon] watching extensions: js,mjs,json [nodemon] starting `babel-node index.js` Your server is running on PORT: 4000 Step 5 - Run the following docker command to check if the container is up and running as a service The container running can also be checked from the docker desktop as well as the number of images that have been created. Make sure that the container that you are running you shut down using the stop command Also, keep a check on the number of images that have been created. These images general run into 1-2 GBs and will very quickly eat up your local disk space. PS C:\\Users\\abhis\\Projects\\docker_poc> docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES cd637f86b4a8 simplebackend \"docker-entrypoint.s\u2026\" 33 seconds ago Up 32 seconds 0.0.0.0:4000->4000/tcp great_heyrovsky PS C:\\Users\\abhis\\Projects\\docker_poc> PS C:\\Users\\abhis\\Projects\\docker_poc> docker stop cd637 cd637 For running a full stack that comprise of frontend , backend, and a database consists of spinning up multiple containers that often form a stack. These containers can be combined to create a service that can be built suing docker compose To look at how to setup a multi-container based full stack service look at the notes Docker Compose To develop a CI pipeline from Github project using Travis CI app refer to the notes on CI with Travis This takes it to full circle where once the code is developed, tested locally, and committed to the git repo in Github a continuous integration tool such as Travis can be used for continuous build that gets triggered for each commit to compile the code and push the image into a remote repository such as docker hub. These notes are based on my understanding of the topic. Some of the content and images refers to the online articles, learning courses I attended. I encourage you to lookup the links provided to develop your own understanding of it at deeper level. The intention of these notes is to serve as reference marker to other sites, online courses, books that I found useful while learning about it. If you would like to share any thoughts or give comments please feel free to reach me @ abhishekgupta86@gmail.com or @ www.linkedin.com/in/abhishekgupta86","title":"Compile simple project in docker"},{"location":"Data%20Management/Snowflake-Vs.-Databricks/","text":"Snowflake Vs. Databricks \u00b6 Created: 2022-06-28 09:47:11 -0400 Modified: 2022-06-29 07:54:58 -0400 Feature Databricks Snowflake Founders Ex-Berkley, Authors of Spark. Donated Spark to Apache and then opened managed services company as Databricks. Planned for IPO in 2022 Ex-Oracle, Oracle principles behind the snowflake architecture Public listed. Revenue $1.2B Architecture Big data distributed computing provided spark as a managed service. SaaS Datawarehouse platform. Truly separates storage from compute. Implementation Cloud Tight with Azure but also available on GCP, AWS Tight with AWS but also available on GCP, Azure Machine Learning ML Lib library. Native to the Spark distribution. ML Core is the core library that does internal management, scheduling, memory management. Jupyter Notebook style environment to perform the ML experiments. Provides machine learning capability through newly launched feature Snowpark. It is Snowflake + Spark providing connectors through java, scala, python api. It requires the data to be available in the snowflake. Else you need to load the data from lake into snowflake. Partner ecosystem that integrates with snowflake. Data Ingestion ML Streams. Part of the Spark core distribution. Provides both real-time events and batch ELT processing. Loads the data from various data sources including data lake. Uses partner network to provide the ingestion. Databricks is also a partner of snowflake Has developed snowpipe for short batch mode data ingestion Use cases Focused on Machine learning use cases Big data processing Focused on data warehousing use cases Developing dashboards on top of processed data from snowflake Pricing Pay as you go pricing. You pay for the databricks and the underlying resources such as the compute and storage used in the cloud Pay as you go. You pay for the storage of the data and the compute. This is in form a warehouse servers which can be spin up to perform the compute. Scalability Clusters can be scaled with 100's of spark executor nodes added to the cluster to perform the compute. Clusters can be spin on for each job to be executed and then shut down the job completes. Cluster can also be spin up for performing ML development Has auto scaling mode. Warehouse machines are small, medium, large, X Large. Each has defined number of nodes that can be increased or decreased in auto-scaling mode. Users Data Engineers, Data Scientist, Advanced Data analysts Data analysts, business users, Data engineers Coding languages Java, Scala (Major), Pyspark(Major), C, Hive SQL ANSI SQL, Python (Snowpark. New feature), Scala (Snowpark. New feature)","title":"Snowflake Vs. Databricks"},{"location":"Data%20Management/Snowflake-Vs.-Databricks/#snowflake-vs-databricks","text":"Created: 2022-06-28 09:47:11 -0400 Modified: 2022-06-29 07:54:58 -0400 Feature Databricks Snowflake Founders Ex-Berkley, Authors of Spark. Donated Spark to Apache and then opened managed services company as Databricks. Planned for IPO in 2022 Ex-Oracle, Oracle principles behind the snowflake architecture Public listed. Revenue $1.2B Architecture Big data distributed computing provided spark as a managed service. SaaS Datawarehouse platform. Truly separates storage from compute. Implementation Cloud Tight with Azure but also available on GCP, AWS Tight with AWS but also available on GCP, Azure Machine Learning ML Lib library. Native to the Spark distribution. ML Core is the core library that does internal management, scheduling, memory management. Jupyter Notebook style environment to perform the ML experiments. Provides machine learning capability through newly launched feature Snowpark. It is Snowflake + Spark providing connectors through java, scala, python api. It requires the data to be available in the snowflake. Else you need to load the data from lake into snowflake. Partner ecosystem that integrates with snowflake. Data Ingestion ML Streams. Part of the Spark core distribution. Provides both real-time events and batch ELT processing. Loads the data from various data sources including data lake. Uses partner network to provide the ingestion. Databricks is also a partner of snowflake Has developed snowpipe for short batch mode data ingestion Use cases Focused on Machine learning use cases Big data processing Focused on data warehousing use cases Developing dashboards on top of processed data from snowflake Pricing Pay as you go pricing. You pay for the databricks and the underlying resources such as the compute and storage used in the cloud Pay as you go. You pay for the storage of the data and the compute. This is in form a warehouse servers which can be spin up to perform the compute. Scalability Clusters can be scaled with 100's of spark executor nodes added to the cluster to perform the compute. Clusters can be spin on for each job to be executed and then shut down the job completes. Cluster can also be spin up for performing ML development Has auto scaling mode. Warehouse machines are small, medium, large, X Large. Each has defined number of nodes that can be increased or decreased in auto-scaling mode. Users Data Engineers, Data Scientist, Advanced Data analysts Data analysts, business users, Data engineers Coding languages Java, Scala (Major), Pyspark(Major), C, Hive SQL ANSI SQL, Python (Snowpark. New feature), Scala (Snowpark. New feature)","title":"Snowflake Vs. Databricks"}]}